{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"../Dataset/\"\n",
    "files = os.listdir(dataset_dir)\n",
    "\n",
    "dataset_files = [ os.path.join(dataset_dir, file) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = {}\n",
    "for dataset_filename in dataset_files:\n",
    "    abc_notation_file = open(dataset_filename, 'r')\n",
    "    songs[os.path.basename(dataset_filename)] = abc_notation_file.read()\n",
    "    abc_notation_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File to train\n",
    "train_list = list(songs.keys())\n",
    "musical_train_file = train_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin training our RNN model, we'll need to create a numerical representation of our text-based dataset. To do this, we'll generate two lookup tables: one that maps characters to numbers, and a second that maps numbers back to characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are 83 unique characters in the dataset\n"
    }
   ],
   "source": [
    "# Find all unique characters in the joined string\n",
    "vocab = sorted(set(songs[musical_train_file]))\n",
    "print(\"There are\", len(vocab), \"unique characters in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from character to unique index.\n",
    "# For example, to get the index of the character \"d\", \n",
    "#   we can evaluate `char2idx[\"d\"]`.  \n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "\n",
    "# Create a mapping from indices to characters. This is\n",
    "#   the inverse of char2idx and allows us to convert back\n",
    "#   from unique index to the character in our vocabulary.\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{\n  '\\n':   0,\n  ' ' :   1,\n  '!' :   2,\n  '\"' :   3,\n  '#' :   4,\n  \"'\" :   5,\n  '(' :   6,\n  ')' :   7,\n  ',' :   8,\n  '-' :   9,\n  '.' :  10,\n  '/' :  11,\n  '0' :  12,\n  '1' :  13,\n  '2' :  14,\n  '3' :  15,\n  '4' :  16,\n  '5' :  17,\n  '6' :  18,\n  '7' :  19,\n  ...\n}\n"
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_string(string):\n",
    "  vectorized_output = np.array([char2idx[char] for char in string])\n",
    "  return vectorized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[49 22 14 ... 22 82  2]\n"
    }
   ],
   "source": [
    "print(vectorize_string(songs[musical_train_file]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "200425\n200425\n"
    }
   ],
   "source": [
    "print(len(songs[musical_train_file]))\n",
    "print(len(vectorize_string(songs[musical_train_file])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicalDataset(torch.nn.Module):\n",
    "    def __init__(self, abc_string, seq_lenght):\n",
    "        self.dataset = abc_string\n",
    "        self.seq_lenght = seq_lenght\n",
    "\n",
    "        vocab = self.vocabulary(abc_string)\n",
    "        \n",
    "        self.char2idx, self.idx2char = self.mapping(vocab)\n",
    "        \n",
    "        self.vectorized_dataset = self.vectorize_string(self.dataset)\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "            Why -1...?\n",
    "            Suppose seq_length is 4 and our text is \"Hello\". Then, our\n",
    "            input sequence (x) is \"Hell\" and the target sequence (y) is \"ello\".\n",
    "        '''\n",
    "        return len(self.dataset) - self.seq_lenght - 1\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.vectorized_dataset[idx : idx + self.seq_lenght]\n",
    "        y = self.vectorized_dataset[idx + 1 : idx + self.seq_lenght + 1]\n",
    "\n",
    "        return [torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)]\n",
    "    \n",
    "    def vectorize_string(self, string):\n",
    "        '''\n",
    "            Vectorize (convert to numerical) a string using the\n",
    "            mapping created by the notation presented in the dataset.\n",
    "            \n",
    "            @return a numpy array with `N` elements, where `N` is\n",
    "                the number of characters in the input string\n",
    "        '''\n",
    "        return np.array([char2idx[char] for char in string])\n",
    "    \n",
    "    def vocabulary(self, string):\n",
    "        '''\n",
    "            Return the vocabulary used in the input string, i.e\n",
    "            a set of no duplicated elements.\n",
    "            \n",
    "            @param string: the dataset with several songs written using\n",
    "                a specific anotation\n",
    "        '''\n",
    "        return sorted(set(string))\n",
    "    \n",
    "    def mapping(self, vocab):\n",
    "        '''\n",
    "         Create a mapping from character to unique index and from \n",
    "         indices to characters. \n",
    "         \n",
    "         @param vocab: A set with no duplicate elements which represents\n",
    "             the vocabulary of our anotation (all unique characters).\n",
    "         @return Mapping contained in a list [char2idx, idx2char]        \n",
    "        '''\n",
    "        char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "        idx2char = np.array(vocab)\n",
    "        \n",
    "        return [char2idx, idx2char]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "200416\n100208\ntensor([[27, 15,  1, 26, 82, 32, 15, 22],\n        [15,  1, 26, 82, 32, 15, 22, 82]])\n"
    }
   ],
   "source": [
    "test = MusicalDataset(songs[musical_train_file], 8)\n",
    "print(len(test))\n",
    "\n",
    "dataloader = DataLoader(test, batch_size=2, shuffle=False, num_workers=0)\n",
    "print(len(dataloader))\n",
    "\n",
    "idx = 0\n",
    "for inputs, targets in dataloader:\n",
    "    idx += 1\n",
    "\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([2, 8])\n"
    }
   ],
   "source": [
    "x, y = next(iter(dataloader))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Recurrent Neural Network (RNN) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is based off the LSTM architecture, where we use a state vector to maintain information about the temporal relationships between consecutive characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/aamini/introtodeeplearning/2019/lab1/img/lstm_unrolled-01-01.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesante añadir una descripción del funcionamiento de los *emmbeding layers* para que quede claro su funcionamiento. \n",
    "\n",
    "Arreglar la imagen para que quede más explicito las dimensiones entre capas.\n",
    "\n",
    "Añadir los detalles de los pasos del LSTM: olvidar, añadir..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MusicalLSTMModel(torch.nn.Module):\n",
    "    '''Container module with an encoder, a recurrent module, and a decoder.'''\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_state_dim, rnn_units, dropout=0.05):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.encoder = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = torch.nn.LSTM(embedding_dim, hidden_state_dim, rnn_units, dropout=dropout)\n",
    "        self.decoder = nn.Linear(hidden_state_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        emb = self.encoder(x)\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.decoder(output)\n",
    "        return output, hidden"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}